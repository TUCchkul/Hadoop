
###Copy from local 
cp /home/cloudera/depart_data.csv /tmp/hive_class_2/
-----------------load data from local to hive----------------
load data local inpath 'file:///home/cloudera/hive_class2/dept_data.csv' into table department_data;
           --whenever we work with local file system we need to help hive in order to pass the file path successfully 
           --because its try to find the path in local system.
           --Since its local file system URI for local file system will be different so we need to add file://
select count(*) from department_data;
#Display column name
set hive.cli.print.header=true;
---------------------------load data hadoop to Hive------------------
load data inpath '/tmp/hive_class_2/' into table department_data_from_hdfs;

Create external table in HIVE
create external table department_data_external       
    > (                                                                                                                  
    > dept_id int,                                                                                                                            
    > dept_name string,                                                                                                                       
    > manager_id int,                                                                                                                         
    > salary int                                                                                                                              
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > location '/tmp/hive_data_class_2/'; 

Internal table: Metada in hive and original raw data in hive
External tables only metadata in Hive

# work with Array data types

create table employee                                                                                                                   
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > skills array<string>                                                                                                                    
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > collection items terminated by ':';                                                                                                     
###load data from hive
load data local inpath 'file:///tmp/hive_class/array_data.csv' into table employee; 

# Get element by index in hive array data type

select id, name, skills[0] as prime_skill from employee;

select                                                                                                                                  
    > id,                                                                                                                                     
    > name,                                                                                                                                   
    > size(skills) as size_of_each_array,                                                                                                     
    > array_contains(skills,"HADOOP") as knows_hadoop,                                                                                        
    > sort_array(skills) as sorted_array                                                                                                                     
    > from employee; 
    
####################How tocreate  back up table tada---
create table sales_table_backup  as select * from sales_data_v2;
###################details about the table############
describe extended sales_table_backup;
# create a table which will store data in parquet
create table sales_data_pq_final                                                                                                        
    > (                                                                                                                                       
    > product_type string,                                                                                                                    
    > total_sales int                                                                                                                         
    > )                                                                                                                                       
    > stored as parquet;  
    
#####load data in parquet file#####
from sales_data_v2 insert overwrite table sales_data_pq_final select *;
